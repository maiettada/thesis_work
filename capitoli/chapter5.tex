% !TEX encoding = IsoLatin 

% Affinché gli accenti vengano accettati, assicurati che la codifica di questo file
% sia ISO 8859-1

% PER OTTENERE IL PDF, digitare da terminale
% ./makepdfplease
% 
\chapter[Valutazione dei risultati][]{5. Valutazione dei risultati}

Dopo l'implementazione dei due prototipi, possiamo introdurre la loro valutazione con l'uso della classe Scorer della libreria SpaCy.
La terminologia Scorer proviene dal mondo del Machine Learning ed indica l'affidabilità della predizione di una data entità in termini di \textbf{precision}, \textbf{recall} e \textbf{accuracy}.

\section{Scorer Class}
Lo Scorer è la classe fornita da SpaCy per valutare le performance di un modello annotatore per ogni singola entità basandosi sulle 
annotazioni prodotte da tale modello.
Lo Scorer richiede all'utente della libreria di avere a disposizione l'output del modello annotatore e le gold annotations della Ground Truth.
Per ogni annotazione si crea un oggetto spacy.Span e si crea una lista di Span, che chiameremo labelled\_entities\_span\_list; si potrà poi utilizzare la lista di gold annotations per creare il relativo \textbf{item} che descrive l'annotazione di un documento:

\begin{lstlisting}[language=Python, caption=Esempio di item]
item = Example.from_dict(labelled_entities_span_list, 
                         {"entities": [[start_offset, end_offset, word]
                                      for [start_offset, end_offset, word]
                                      in gold_annots]})
\end{lstlisting}

Si crea un Example per ogni documento annotato e si pongono gli item in una \textbf{item\_list}.
Tutti i dati annotati dal modello annotatore sono ora nella item list ed è dunque possibile invocare 
il task di score vero e proprio:
\begin{lstlisting}[language=Python]
    scores = scorer.score(item_list)
\end{lstlisting}

\section{Confronto tra i due approcci}
Nella tabella scores-comparisons.pdf riporta i valori di Precision, Recall e Accuracy ottenuti per ogni entità contemplata nel task di 
classificazione.


%allora\includegraphics[width=16cm, height=30cm,page=1]{scores-comparisons.pdf} %width=, height=, page=,
\includegraphics[trim=0 300 0 0, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{confronti.pdf} 
\captionof{figure}{confronto tra l'approccio regex(sinistra) e NN(destra)}
Si può facilmente notare come molte entità di tipo \textbf{Categoria} vedono valori di precision recall e accuracy migliori nell'approccio Machine Learning; questo miglioramento si rivela significativo in circa il 60\% delle entità categoria.
Nel restante 40\% delle categorie si registra un peggioramento delle performance del Machine Learning, che in pochi ma significativi casi abbatte a zero le score.
Questo può essere dovuto alla mancanza di esempi numerosi per tutte le entità: 
ciò porterebbe il modello a funzionare molto bene nel riconoscere le entità ben rappresentate dalla ground truth,
ma anche a funzionare peggio per tutte le entità meno note.
Le regex invece, fornendo a priori i pattern di ogni entità, non soffrono della mancanza di esempi. 
\section{Confronto tra i due prototipi}
Come sottolineato precedentemente, l'approccio NER con regex non ha grandi margini di miglioramento dell'accuratezza:
in sintesi è un approccio rigido, che in caso di inclusione di nuovi pattern testuali 
richiederebbe la continua modifica di una regex in forme sempre più complesse, risolvendosi in
problemi di manutenzione, di testing, di leggibilità e talvolta di sicurezza.
Nel caso del Machine Learning un task di \acrfull{named-entity-recognition} può avere risultati migliori delle regex, 
ma può soffrire di carenza di esempi per alcune entità.
Su questa base si potrebbe proporre l'uso combinato degli output di entrambi i modelli: in particolare,
si potrebbe ricorrere al modello regex per le entità meno assimilate dal modello \acrfull{machine-learning}
, preferendo invece quest'ultimo
per le entità su cui gli score descrivono prestazioni migliori.



\section{Human In The Loop}
Fin qui il lavoro di \acrshort{societa-soa} extraction ci ha portati a considerare due tecnologie molto diverse per
approccio e manutenibilità.

In particolare, le \textbf{regex} risultano di risultato immediato poiché in pochi minuti ci consentono di
svolgere un'estrazione di dati SOA con discreti valori di recall,
ma soffrono di \textbf{scarsa leggibilità}, manutenzione problematica e decisamente \textbf{error-prone} ;
nonostante un incoraggiante risultato immediato, non permettono di migliorare in maniera efficiente l'estrazione di 
entità per tutta una serie di fattori.
Consideriamo ad esempio il manutentore a cui toccherà estendere la regex in produzione \(reg\_prod\sb{n}\); 
il suo compito sarà tipicamente quello di trasformare la \(reg\sb{prod}\) in una \(reg\_prod\sb{n+1}\)
in maniera da includere i nuovi pattern \( p\sb{m+1} , \ldots , p\sb{m+k} \).
Le domande che ci poniamo sono allora le seguenti:
Quella vecchia regex \(reg\_prod\sb{n}\) sarà stata fino ad allora documentata per tenere traccia dei
pattern precedenti \( p\sb{1}, \ldots ,  p\sb{m} \) ?
Oppure spetterà al programmatore ricordare tutti i singoli pattern che venivano catturati con la precedente versione della regex?
E la regex avrà dei dati di \textbf{testing}, per verificare che la nuova versione \(reg\_prod\sb{n+1}\)
non comprometta i pattern fino ad allora accettati?
Tutte queste domande si ritrovano nell'indagine statistica di \say{Regexes are hard} \cite{RegexesAreHard}, 
dove la risposta è negativa: le regex vengono usate perlopiù senza documentazione e senza dati di test,
per cui un sistema basato su sole regex è nel tempo inefficiente nel miglioramento della \acrlong{Precision}.

Al contrario, le reti neurali possono nel tempo \say{imparare meglio} le entità con cui hanno a che fare ma richiedono un consistente lavoro umano di annotazione e classificazione delle entità presenti nei dati stessi;
in altre parole, mancano dell'immediatezza delle regex ma rendono fattibile il \textbf{fine-tuning} del sistema classificatore.

%\includegraphics[trim=0 300 0 0, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{grafo_regex.pdf}
%\includegraphics[trim=0 300 0 0, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{grafo_nn.pdf}
%\includegraphics[trim=0 300 0 0, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{grafo_regexnn.pdf} 

I rispettivi punti di forza e di debolezza delle due metodologie risultano complementari, per cui ha senso combinarli in un
unico sistema che sfrutti i vantaggi di entrambe, mitigandone gli svantaggi.


%\includegraphics[trim=0 300 0 0, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{grafo_hil.pdf} 

Possiamo dunque configurare l'uso delle componenti regex, \acrshort{machine-learning}, task di creazione regex e task di 
annotazione manuale come un sistema di tipo \acrshort{human-in-the-loop}.
In tale sistema l'essere umano piò essere utilizzato come programmatore di regex e come annotatore a più riprese successive.
Partendo dallo studio \say{How to invest my time} \cite{HowToInvestMyTime}, l'indicazione che traiamo per usare al meglio
il lavoro umano è investire pochi minuti sulla creazione di regex ad alta \acrlong{Recall} per poi far seguire a questa  la correzione umana delle annotazioni prodotte dalla regex stessa.
Seguendo la convenzione dello studio citato, chiameremo \say{weak labelling} l'estrazione svolta con la regex, che indicheremo con \(RE\sb{WL}\).
La \textbf{human annotation} prenderà in input le \textbf{weak labels} prodotte con la  \(RE\sb{WL}\)
e produrrà annotazioni che potremo considerare  affidabili perché verificate da un umano;
tali annotazioni costituiranno il training set della rete neurale.




