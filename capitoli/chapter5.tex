% !TEX encoding = IsoLatin 

% Affinché gli accenti vengano accettati, assicurati che la codifica di questo file
% sia ISO 8859-1

% PER OTTENERE IL PDF, digitare da terminale
% ./makepdfplease
% 
\chapter[Valutazione dei risultati][]{5. Valutazione dei risultati}

Dopo l'implementazione dei prototipi NER-regex e NER-ML, possiamo introdurre la loro valutazione con l'uso di un apposito Scorer.
Dato un sistema NER, lo Scorer fornisce delle misure di affidabilità dell'NER stesso in termini di \textbf{precision}, \textbf{recall} e \textbf{accuracy}.



\section{Confronto tra regex e ML}
Nella tabella scores-comparisons.pdf \ref{fig:v1-nums} riportiamo i valori di Precision, Recall e Accuracy ottenuti per ogni entità contemplata nel task di classificazione, sia nel caso di NER-regex che nel caso NER-ML.
Le regex considerate sono le seguenti:
 \[regex-soa\sb{wl-v1}=\textsc{\char13}O(S|G)(\\d\\d?)\textsc{\char13} \]  \[ regex-class\sb{wl-v1}=\textsc{\char13}(IV|IV|III|II|IX|VIII|VII|VI|X|V|I)\textsc{\char13} \]



Si può facilmente notare come molte entità di tipo \textbf{Categoria} vedono valori di precision recall e accuracy migliori nell'approccio Machine Learning; in particolare sono stati evidenziati in verde i valori di precision, recall ed f1-score che in NER-ML superano almeno dello 0.05 i corrispondenti valori registrati dall'NER-regex. 
In un caso (entity type OG-4) si registra un peggioramento delle performance del Machine Learning, che abbatte a zero le score.
Questo può essere spiegato con la mancanza di esempi numerosi per alcune entità: 
ciò porterebbe il modello a funzionare molto bene nel riconoscere le entità ben rappresentate dalla ground truth,
ma anche a funzionare peggio per tutte le entità meno note.

\includegraphics[trim=0 300 0 0, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{ConfrontiNumerici+ - v1.pdf} 
\captionof{figure}{confronto tra l'approccio regex(sinistra) e NN(destra)}
\label{fig:v1-nums}

Come sottolineato precedentemente, l'approccio NER-regex non ha grandi margini di miglioramento dell'accuratezza:
in sintesi è un approccio rigido, che in caso di inclusione di nuovi pattern testuali 
richiederebbe la continua modifica di una regex in forme sempre più complesse, risolvendosi in
problemi di manutenzione, di testing, di leggibilità e talvolta di sicurezza.
Nel caso del Machine Learning un task di \acrfull{named-entity-recognition} può avere risultati migliori delle regex, 
anche se può inizialmente soffrire di scarsa numerosità di entities.
Su questa base si potrebbe proporre l'uso combinato degli output di entrambi i modelli: in particolare,
si potrebbe ricorrere al modello regex per le entità meno assimilate dal modello \acrfull{machine-learning}
, preferendo invece quest'ultimo per le entità su cui gli score descrivono prestazioni migliori.

Nell'istogramma in figura \ref{fig:v1-hist} si confrontano le medie di P,R,F1 nel caso della regex e nel caso
dell'NER-ML. Le regex fin qui considerate sono regex non complicate, possono certamente essere migliorate per coprire
una casistica di scritture più estesa: prevedendo degli spazi aggiuntivi, prevedendo separatori alternativi e prevedendo 
l'uso combinato di lettere maiuscole e minuscole.
Tuttavia, come sottolineato precedentemente, caratteristica fondamentale dei dati è essere usati nella letteratura 
burocratica: in quest'ambito, colui che scrive il documento formale può ad esempio scrivere \say{OG4}, ma anche \say{Opera Generale numero quattro} e ancora \say{Op.Gen.num.4}, per cui risulta ingiustificato investire tempo per ottenere una regex \say{ottimale}.
Un'alternativa potrebbe nascere dalla combinazione dell'uso del NER-regex e del NER-ML.

\includegraphics[clip, width=0.6\textwidth,height=0.6\textheight,keepaspectratio, page=1]{regex-v1-vs-ml.pdf} 
%	\captionof{figure}{confronto tra l'approccio regex(sinistra) e NN(destra)}
\caption{Da sinistra: Valori di P,R,F del NER-regex-v1 e del NER-ML}
\label{fig:v1-hist}


\section{Human In The Loop}
Fin qui il lavoro di \acrshort{societa-soa} extraction ci ha portati a considerare due tecnologie molto diverse per
approccio e manutenibilità.

In particolare, le \textbf{regex} risultano di risultato immediato poiché in pochi minuti ci consentono di
svolgere un'estrazione di dati SOA con discreti valori di recall,
ma soffrono di \textbf{scarsa leggibilità}, manutenzione problematica e risultano perciò decisamente \textbf{error-prone};
nonostante un incoraggiante risultato immediato, non permettono di migliorare in maniera efficiente l'estrazione di 
entità per tutta una serie di fattori.
Consideriamo ad esempio il manutentore a cui toccherà estendere la regex in produzione \(reg\_prod\sb{n}\): 
il suo compito sarà tipicamente quello di trasformare la \(reg\_prod\sb{n}\) in una \(reg\_prod\sb{n+1}\)
in maniera da includere i nuovi pattern \( p\sb{m+1} , \ldots , p\sb{m+k} \).
Le domande che ci poniamo sono allora le seguenti:
\begin{itemize}
\item
Quella vecchia regex \(reg\_prod\sb{n}\) sarà stata fino ad allora documentata per tenere traccia dei
pattern precedenti \( p\sb{1}, \ldots ,  p\sb{m} \) ?
\item Oppure spetterà al programmatore ricordare tutti i singoli pattern che venivano catturati con la precedente versione della regex?
\item
E la regex avrà dei dati di \textbf{testing}, per verificare che la nuova versione \(reg\_prod\sb{n+1}\)
non comprometta i pattern fino ad allora accettati?
\end{itemize}
Tutte queste domande si ritrovano nell'indagine statistica di \say{Regexes are hard} \cite{RegexesAreHard}, 
dove emerge che le best practices dell'uso delle regex sono sistematicamente disattese:
le regex vengono scritte perlopiù senza aggiungere documentazione e senza dati di test.
Tutte queste problematiche nel complesso impediscono ad un sistema  basato su regex di essere efficiente nel miglioramento della \acrlong{Precision}.

Al contrario, le reti neurali possono nel tempo \say{imparare meglio} le entità con cui hanno a che fare ma richiedono un consistente lavoro umano di annotazione e classificazione delle entità presenti nei dati stessi;
in altre parole, mancano dell'immediatezza delle regex ma rendono fattibile il \textbf{fine-tuning} del sistema classificatore.

%\includegraphics[trim=0 300 0 0, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{grafo_regex.pdf}
%\includegraphics[trim=0 300 0 0, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{grafo_nn.pdf}
%\includegraphics[trim=0 300 0 0, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{grafo_regexnn.pdf} 

I rispettivi punti di forza e di debolezza delle due metodologie risultano complementari, per cui ha senso combinarli in un
unico sistema che sfrutti i vantaggi di entrambe, mitigandone gli svantaggi.

% HIL DIAGRAM
\includegraphics[trim= 10 200 0 100, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{flow_chart_hil.pdf} 
\caption{Framework HITL: fase di addestramento} 
\begin{comment}

	\begin{figure}[ht] % placed here or on the top of page
		\begin{tikzpicture}[node distance=3cm]
		%\begin{tikzpicture}%[->,shorten >=1pt,auto,node distance=2.8cm,semithick]
			\node (data) [state] {Data};
			\node (module0) [process, below=1cm of data] {regex\_ner};
			\draw [arrow] (data) -- (module0);
			\node (wl) 		[state, below=1cm of module0] {wl};
			\draw [arrow] (module0) -- (wl);
			\node (humcorr) [process, below=1cm of wl] {human\_correc};
			\draw [arrow] (wl) -- (humcorr);
			\node (l) 		[state, left of=humcorr] {labels};
			\draw [arrow] (humcorr) -- (l);
			\node (train) 	[process, below of=l] {training};
			\draw [arrow] (l) -- (train);
			\node (nn) 		[state, below of=train] {nn};
			\draw [arrow] (train) -- (nn);
			\node (module) [process, right of=nn] {nn\_usage};
			\draw [arrow] (nn) -- (module);
			\node (entities) [state, below of=module] {Entities};
			\node (entitiescorr) [state, above of=module] {Entities};
			\draw [arrow] (module) -- (entities);
			\draw [arrow] (module) -- (entitiescorr);
			\draw [arrow] (entitiescorr) -- (humcorr);
			
\begin{pgfonlayer}{background}
\filldraw [fill=black!30,draw=red] (3,-5) rectangle (-5,-13.5);
%\filldraw [fill=black!30,draw=red] (nn.south -| nn.west) rectangle (humcorr.north -| entitiescorr.east);
\end{pgfonlayer}
		\end{tikzpicture}
		\caption{Human In The Loop framework} % caption
		%\label{fig:diagram}             % for referencing of figure, key select as you wish
	\end{figure}
\end{comment}



Configuriamo l'uso delle componenti regex, \acrshort{machine-learning}, task di creazione regex e task di 
annotazione manuale come un sistema di tipo \acrshort{human-in-the-loop}.
In tale sistema l'essere umano può essere utilizzato come programmatore di regex e come annotatore a più riprese successive.
Partendo dallo studio \say{How to invest my time} \cite{HowToInvestMyTime}, l'indicazione che traiamo per usare al meglio
il lavoro umano è investire pochi minuti sulla creazione di regex ad alta \acrlong{Recall} per poi effettuare la correzione umana delle annotazioni prodotte dalla regex stessa.
Seguendo la convenzione dello studio citato, chiameremo \say{weak labelling} l'estrazione svolta con la regex, che indicheremo con \(RE\sb{WL}\): le label così prodotte risultano \say{weak}, ossia non verificate e passibili d'errore.
%La \textbf{human annotation} prenderà in input le \textbf{weak labels} prodotte con la  \(RE\sb{WL}\)
%e produrrà annotazioni che potremo considerare affidabili perché verificate da un umano;
%tali annotazioni costituiranno il training set della rete neurale.
L'attività umana \textbf{\(correct\sb{k}labels\)}, partendo dalle weak labels come input iniziale \(labels\sb{0}\), ad ogni iterazione \textit{i} apporterà \textit{k} correzioni che andranno a formare l'insieme di labels \(labels\sb{i}\). Qualora il ciclo continui, tale insieme i-esimo potrà essere adoperato come input dell'iterazione \textit{i+1}; in caso contrario verrà assunto come training set per istruire la rete neurale.


%\begin{left}
\includegraphics[ clip, width=\textwidth,height=\textheight,keepaspectratio]{PlotFunctionData_30+.pdf} 
\caption{Human In The Loop: F1-score, Precision e Recall.} 
\label{fig:hitl}
%\end{left}

Il risultato del loop è l'addestramento della rete neurale n-esima \(nn\sb{n}\) utilizzabile per effettuare il task di 
\acrshort{named-entity-recognition}.


L'uso dello \acrlong{human-in-the-loop} può essere vantaggioso se effettuato in maniera tale da:
\begin{itemize}
\item istruire una rete neurale in un tempo minimo di lavoro umano;  
\item permettere il fine-tuning della rete neurale stessa
con l'intervento dell'annotatore umano.
\end{itemize}

Seguendo dunque lo studio già citato, l'operatore umano deve destinare pochi minuti alla realizzazione delle  regex \(RE\sb{WL}\). Nella nostra prova utilizziamo le regex \[regex-soa\sb{wl-v1}=\textsc{\char13}O(S|G)(\\d\\d?)\textsc{\char13} \]  \[ regex-class\sb{wl-v1}=\textsc{\char13}(IV|IV|III|II|IX|VIII|VII|VI|X|V|I)\textsc{\char13} \] realizzate in pochi minuti su un tool di regex-editing liberamente disponibile online. Usando tali regex abbiamo quindi prodotto le weak labels che vanno a formare il dataset iniziale \(labels\sb{0}\);
per ogni iterazione successiva \(i>0\) vengono 
 dunque corrette \textit{k} annotazioni weak con \(k=100\). Il grafico \ref{fig:hitl} illustra l'evoluzione del sistema \acrshort{human-in-the-loop} all'aumentare 
delle correzioni umane apportate alle weak labels: viene
rappresentato il guadagno in termini di \acrlong{Precision}, \acrlong{Recall} e \acrlong{F1-score}.
In vari esperimenti analoghi a quello riportato in figura \ref{fig:hitl} si evince come in dieci iterazioni di correzione il %sistema guadagni oltre il 22\% di \acrlong{Precision}, 34\% di \acrlong{Recall} e 32\% di \acrlong{F1-score}.
sistema valutato in termini di  \acrshort{Precision}, \acrshort{Recall}, \acrshort{F1-score} riporti i guadagni minimi:
\[\Delta\sb{i=10}(P,R,F1)=(22\%,34\%, 32\%)\]
È interessante notare come alla settima iterazione correttiva tali guadagni ammontano già al
\[\Delta\sb{i=7}(P,R,F1)=(21\%,33\%, 31\%)\]


\center
\includegraphics[]{table_deltas.pdf}
%\includegraphics[]{PlotFunctionData_hitl_vs_ml_notitle.pdf}
\caption{I valori di p,r,e f1 di NER-HITL-v1 e scarti rispetto a NER-ML}
\label{fig:ist}
\center

Portando il discorso ai risultati specifici dell'esperimento al grafico \ref{fig:hitl}, l'iterazione numero dieci porta ai valori \[[P,R,F]\sb{NER-HITL}=[0.8458, 0.7071, 0.7527 ] \] a fronte dei valori 
del sistema NER-ML costruito sulla \textit{ground truth} \[[P,R,F]\sb{NER-ML}= [0.8476, 0.7778, 0.7973] \]. 
Possiamo osservare lo stesso specifico esperimento  alla tabella \ref{fig:ist}, dove forniamo anche 
un calcolo degli scarti tra i valori del NER-ML e i valori dello HITL ad ogni iterazione \textit{i}.
%width=\textwidth,height=\textheight,keepaspectratio]{PlotFunctionData_hitl_vs_ml.pdf} \caption{} \label{fig:hitlvsml}
Si nota come le prestazioni di  \(NER-HITL\sb{n}\) tendano a migliorare all'aumentare delle iterazioni \(n\).

\raggedright
\paragraph{I dataset adoperati}
Nel caso ML sono state necessarie 2218 annotazioni gold prodotte da un annotatore umano;
di questo numero di annotazioni totale, 1500 vanno a costituire il training set ML.
Considerando invece il caso del sistema HITL, il training set adopera 1428 annotazioni gold, 
di cui 1000 sono impiegate al fine di
correggere le annotazioni precedentemente prodotte con regex ( 1315 annotazioni prodotte con regex).
Dunque l'esperimento HITL adopera internamente 790 annotazioni in meno.
Va considerato però il tempo di produzione delle annotazioni gold e silver:
se la produzione di un'annotazione gold richiede 14  secondi,
la correzione di un'annotazione silver richiede soli 6,52 secondi mediamente.
Confrontando dunque l'impiego di tempo dei due tipi di annotazione:
2218*14 nel caso ML
1300*6.52 nel caso HITL
risulta che abbiamo risparmiato 2218*14-1300* 6.52 = più di 6 ore per la produzione del dataset.
In questo senso, ricorrere al sistema HITL fa risparmiare una percentuale pari al 72\% del tempo totale di annotazione.


\paragraph{HITL-14}
Proseguendo il ciclo di addestramento del sistema HITL, nell'esperimento descritto in figura \ref{fig:hitl} si ottiene il superamento delle prestazioni del NER-ML all'iterazione \(i=14\).
Si registrano i  valori:
\[p=0.8823, r=0.7723, f=0.8141\]
Per raggiungere l'iterazione 14, è necessario fornire 2000 annotazioni human-corrected.
La loro produzione richiede un tempo pari a 2000*6.52, che confrontato a annot-time-ML=2218*14 costituisce un
risparmio del 58\% del tempo di annotazione del NER-ML.



% ML 0.847669252797008	0.777828714411617	0.797399765346802
% p 21 r 33 f 31 segna come iterazione 7
% p 25 r 34 f 32 (_v1_29_n)
% p 22 r 38 f 35 (_v1_30_n+)
% p 25 r 36 f 34 (_v1_30_n)
% hitl p 0.845849682049472 r 0.707145200862464 f 0.752779411712778
% gold p 0.847669252797008 r 0.777828714411617 f 0.797399765346802



