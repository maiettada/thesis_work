% !TEX encoding = IsoLatin 

% Affinché gli accenti vengano accettati, assicurati che la codifica di questo file
% sia ISO 8859-1

% PER OTTENERE IL PDF, digitare da terminale
% ./makepdfplease
% 
\chapter[Valutazione dei risultati][]{5. Valutazione dei risultati}

Dopo l'implementazione dei due prototipi, possiamo introdurre la loro valutazione con l'uso della classe Scorer della libreria SpaCy.
La terminologia Scorer proviene dal mondo del Machine Learning ed indica l'affidabilità della predizione di una data entità in termini di \textbf{precision}, \textbf{recall} e \textbf{accuracy}.

\section{Scorer Class}
Lo Scorer è la classe fornita da SpaCy per valutare le performance di un modello annotatore per ogni singola entità basandosi sulle 
annotazioni prodotte da tale modello.
Lo Scorer richiede all'utente della libreria di avere a disposizione l'output del modello annotatore e le gold annotations della Ground Truth.
Per ogni annotazione si crea un oggetto spacy.Span e si crea una lista di Span, che chiameremo labelled\_entities\_span\_list; si potrà poi utilizzare la lista di gold annotations per creare il relativo \textbf{item} che descrive l'annotazione di un documento:

\begin{lstlisting}[language=Python, caption=Esempio di item]
item = Example.from_dict(labelled_entities_span_list, 
                         {"entities": [[start_offset, end_offset, word]
                                      for [start_offset, end_offset, word]
                                      in gold_annots]})
\end{lstlisting}

Si crea un Example per ogni documento annotato e si pongono gli item in una \textbf{item\_list}.
Tutti i dati annotati dal modello annotatore sono ora nella item list ed è dunque possibile invocare 
il task di score vero e proprio:
\begin{lstlisting}[language=Python]
    scores = scorer.score(item_list)
\end{lstlisting}

\section{Confronto tra i due approcci}
Nella tabella scores-comparisons.pdf riporta i valori di Precision, Recall e Accuracy ottenuti per ogni entità contemplata nel task di 
classificazione.


%allora\includegraphics[width=16cm, height=30cm,page=1]{scores-comparisons.pdf} %width=, height=, page=,
\includegraphics[trim=0 300 0 0, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{confronti.pdf} 
\captionof{figure}{confronto tra l'approccio regex(sinistra) e NN(destra)}
Si può facilmente notare come molte entità di tipo \textbf{Categoria} vedono valori di precision recall e accuracy migliori nell'approccio Machine Learning; questo miglioramento si rivela significativo in circa il 60\% delle entità categoria.
Nel restante 40\% delle categorie si registra un peggioramento delle performance del Machine Learning, che in pochi ma significativi casi abbatte a zero le score.
Questo può essere dovuto alla mancanza di esempi numerosi per tutte le entità: 
ciò porterebbe il modello a funzionare molto bene nel riconoscere le entità ben rappresentate dalla ground truth,
ma anche a funzionare peggio per tutte le entità meno note.
Le regex invece, fornendo a priori i pattern di ogni entità, non soffrono della mancanza di esempi. 

Come sottolineato precedentemente, l'approccio NER con regex non ha grandi margini di miglioramento dell'accuratezza:
in sintesi è un approccio rigido, che in caso di inclusione di nuovi pattern testuali 
richiederebbe la continua modifica di una regex in forme sempre più complesse, risolvendosi in
problemi di manutenzione, di testing, di leggibilità e talvolta di sicurezza.
Nel caso del Machine Learning un task di \acrfull{named-entity-recognition} può avere risultati migliori delle regex, 
ma può soffrire di carenza di esempi per alcune entità.
Su questa base si potrebbe proporre l'uso combinato degli output di entrambi i modelli: in particolare,
si potrebbe ricorrere al modello regex per le entità meno assimilate dal modello \acrfull{machine-learning}
, preferendo invece quest'ultimo
per le entità su cui gli score descrivono prestazioni migliori.



\section{Human In The Loop}
Fin qui il lavoro di \acrshort{societa-soa} extraction ci ha portati a considerare due tecnologie molto diverse per
approccio e manutenibilità.

In particolare, le \textbf{regex} risultano di risultato immediato poiché in pochi minuti ci consentono di
svolgere un'estrazione di dati SOA con discreti valori di recall,
ma soffrono di \textbf{scarsa leggibilità}, manutenzione problematica e risultano perciò decisamente \textbf{error-prone} ;
nonostante un incoraggiante risultato immediato, non permettono di migliorare in maniera efficiente l'estrazione di 
entità per tutta una serie di fattori.
Consideriamo ad esempio il manutentore a cui toccherà estendere la regex in produzione \(reg\_prod\sb{n}\): 
il suo compito sarà tipicamente quello di trasformare la \(reg\sb{prod}\) in una \(reg\_prod\sb{n+1}\)
in maniera da includere i nuovi pattern \( p\sb{m+1} , \ldots , p\sb{m+k} \).
Le domande che ci poniamo sono allora le seguenti:
\begin{itemize}
\item
Quella vecchia regex \(reg\_prod\sb{n}\) sarà stata fino ad allora documentata per tenere traccia dei
pattern precedenti \( p\sb{1}, \ldots ,  p\sb{m} \) ?
\item Oppure spetterà al programmatore ricordare tutti i singoli pattern che venivano catturati con la precedente versione della regex?
\item
E la regex avrà dei dati di \textbf{testing}, per verificare che la nuova versione \(reg\_prod\sb{n+1}\)
non comprometta i pattern fino ad allora accettati?
\end{itemize}
Tutte queste domande si ritrovano nell'indagine statistica di \say{Regexes are hard} \cite{RegexesAreHard}, 
dove emerge che le best practices dell'uso delle regex sono sistematicamente disattese:
le regex vengono scritte perlopiù senza aggiungere documentazione e senza dati di test.
Tutte queste problematiche nel complesso impediscono ad un sistema  basato su regex di essere efficiente nel miglioramento della \acrlong{Precision}.

Al contrario, le reti neurali possono nel tempo \say{imparare meglio} le entità con cui hanno a che fare ma richiedono un consistente lavoro umano di annotazione e classificazione delle entità presenti nei dati stessi;
in altre parole, mancano dell'immediatezza delle regex ma rendono fattibile il \textbf{fine-tuning} del sistema classificatore.

%\includegraphics[trim=0 300 0 0, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{grafo_regex.pdf}
%\includegraphics[trim=0 300 0 0, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{grafo_nn.pdf}
%\includegraphics[trim=0 300 0 0, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{grafo_regexnn.pdf} 

I rispettivi punti di forza e di debolezza delle due metodologie risultano complementari, per cui ha senso combinarli in un
unico sistema che sfrutti i vantaggi di entrambe, mitigandone gli svantaggi.

% HIL DIAGRAM
\includegraphics[trim= 10 200 0 100, clip, width=\textwidth,height=\textheight,keepaspectratio, page=1]{flow_chart_hil.pdf} 
\caption{Framework HITL: fase di addestramento} 
\begin{comment}

	\begin{figure}[ht] % placed here or on the top of page
		\begin{tikzpicture}[node distance=3cm]
		%\begin{tikzpicture}%[->,shorten >=1pt,auto,node distance=2.8cm,semithick]
			\node (data) [state] {Data};
			\node (module0) [process, below=1cm of data] {regex\_ner};
			\draw [arrow] (data) -- (module0);
			\node (wl) 		[state, below=1cm of module0] {wl};
			\draw [arrow] (module0) -- (wl);
			\node (humcorr) [process, below=1cm of wl] {human\_correc};
			\draw [arrow] (wl) -- (humcorr);
			\node (l) 		[state, left of=humcorr] {labels};
			\draw [arrow] (humcorr) -- (l);
			\node (train) 	[process, below of=l] {training};
			\draw [arrow] (l) -- (train);
			\node (nn) 		[state, below of=train] {nn};
			\draw [arrow] (train) -- (nn);
			\node (module) [process, right of=nn] {nn\_usage};
			\draw [arrow] (nn) -- (module);
			\node (entities) [state, below of=module] {Entities};
			\node (entitiescorr) [state, above of=module] {Entities};
			\draw [arrow] (module) -- (entities);
			\draw [arrow] (module) -- (entitiescorr);
			\draw [arrow] (entitiescorr) -- (humcorr);
			
\begin{pgfonlayer}{background}
\filldraw [fill=black!30,draw=red] (3,-5) rectangle (-5,-13.5);
%\filldraw [fill=black!30,draw=red] (nn.south -| nn.west) rectangle (humcorr.north -| entitiescorr.east);
\end{pgfonlayer}
		\end{tikzpicture}
		\caption{Human In The Loop framework} % caption
		%\label{fig:diagram}             % for referencing of figure, key select as you wish
	\end{figure}
\end{comment}



Configuriamo l'uso delle componenti regex, \acrshort{machine-learning}, task di creazione regex e task di 
annotazione manuale come un sistema di tipo \acrshort{human-in-the-loop}.
In tale sistema l'essere umano può essere utilizzato come programmatore di regex e come annotatore a più riprese successive.
Partendo dallo studio \say{How to invest my time} \cite{HowToInvestMyTime}, l'indicazione che traiamo per usare al meglio
il lavoro umano è investire pochi minuti sulla creazione di regex ad alta \acrlong{Recall} per poi effettuare la correzione umana delle annotazioni prodotte dalla regex stessa.
Seguendo la convenzione dello studio citato, chiameremo \say{weak labelling} l'estrazione svolta con la regex, che indicheremo con \(RE\sb{WL}\): le label così prodotte risultano \say{weak}, ossia non verificate e passibili d'errore.
%La \textbf{human annotation} prenderà in input le \textbf{weak labels} prodotte con la  \(RE\sb{WL}\)
%e produrrà annotazioni che potremo considerare affidabili perché verificate da un umano;
%tali annotazioni costituiranno il training set della rete neurale.
L'attività umana \textbf{\(correct\sb{k}labels\)}, partendo dalle weak labels come input iniziale \(labels\sb{0}\), ad ogni iterazione \textit{i} apporterà \textit{k} correzioni che andranno a formare l'insieme di labels \(labels\sb{i}\). Qualora il ciclo continui, tale insieme i-esimo potrà essere adoperato come input dell'iterazione \textit{i+1}; in caso contrario verrà assunto come training set per istruire la rete neurale.


%\begin{left}
\includegraphics[ clip, width=\textwidth,height=\textheight,keepaspectratio]{PlotFunctionData_30+.pdf} 
\caption{Human In The Loop: F1-score, Precision e Recall.} 
\label{fig:hitl}
%\end{left}

L'uso dello \acrlong{human-in-the-loop} può essere vantaggioso se effettuato in maniera tale da:
\begin{itemize}
\item istruire una rete neurale in un tempo minimo di lavoro umano;  
\item permettere il fine-tuning della rete neurale stessa
con l'intervento dell'annotatore umano.
\end{itemize}

Seguendo dunque lo studio già citato, l'operatore umano deve destinare pochi minuti alla realizzazione delle  regex \(RE\sb{WL}\). Nella nostra prova utilizziamo le regex \[regex-soa\sb{wl-v1}=\textsc{\char13}O(S|G)(\\d\\d?)\textsc{\char13} \]  \[ regex-class\sb{wl-v1}=\textsc{\char13}(IV|IV|III|II|IX|VIII|VII|VI|X|V|I)\textsc{\char13} \] realizzate in pochi minuti su un tool di regex-editing liberamente disponibile online. Usando tali regex abbiamo quindi prodotto le weak labels che vanno a formare il dataset iniziale \(labels\sb{0}\);
per ogni iterazione successiva \(i>0\) vengono 
 dunque corrette \textit{k} annotazioni weak con \(k=100\). Il grafico \ref{fig:hitl} illustra l'evoluzione del sistema \acrshort{human-in-the-loop} all'aumentare 
delle correzioni umane apportate alle weak labels: viene
rappresentato il guadagno in termini di \acrlong{Precision}, \acrlong{Recall} e \acrlong{F1-score}.
In vari esperimenti analoghi a quello riportato in figura \ref{fig:hitl} si evince come in dieci iterazioni di correzione il %sistema guadagni oltre il 22\% di \acrlong{Precision}, 34\% di \acrlong{Recall} e 32\% di \acrlong{F1-score}.
sistema valutato in termini di  \acrshort{Precision}, \acrshort{Recall}, \acrshort{F1-score} riporti i guadagni minimi:
\[\Delta\sb{i=10}(P,R,F1)=(22\%,34\%, 32\%)\]
È interessante notare come alla settima iterazione correttiva tali guadagni ammontano già al
\[\Delta\sb{i=7}(P,R,F1)=(21\%,33\%, 31\%)\]
Portando il discorso ai risultati specifici dell'esperimento al grafico \ref{fig:hitl}, l'iterazione numero dieci porta ai valori \[[P,R,F]\sb{NER-HITL}=[0.8458, 0.7071, 0.7527 ] \] a fronte dei valori 
del sistema NER-ML costruito sulla \textit{ground truth} \[[P,R,F]\sb{NER-ML}= [0.8476, 0.7778, 0.7973] \]. In figura 
\ref{fig:ist} possiamo vedere graficamente il confronto tra \textit{NER-HITL} e \textit{NER-ML}.%\includegraphics[ clip, width=\textwidth,height=\textheight,keepaspectratio]{PlotFunctionData_hitl_vs_ml.pdf} \caption{} \label{fig:hitlvsml}



\center
\includegraphics[width=0.9\linewidth, height=6cm]{PlotFunctionData_hitl_vs_ml.pdf}
\caption{Grafico comparativo di NER-HITL-v1 e NER-ML}
\label{fig:ist}
\center
% ML 0.847669252797008	0.777828714411617	0.797399765346802
% p 21 r 33 f 31 segna come iterazione 7
% p 25 r 34 f 32 (_v1_29_n)
% p 22 r 38 f 35 (_v1_30_n+)
% p 25 r 36 f 34 (_v1_30_n)
% hitl p 0.845849682049472 r 0.707145200862464 f 0.752779411712778
% gold p 0.847669252797008 r 0.777828714411617 f 0.797399765346802
